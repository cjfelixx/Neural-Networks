{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network models using the MNIST dataset\n",
    "The Modified National Institute of Standards and Technology (MNIST) dataset is an image set of handwritten digits, where each image has target values.\n",
    "I will not use data augmentations to focus on the basic algorithms for the NN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the dimensions of the training and testing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial example dimensions\n",
      "x_train shape (60000, 28, 28)\n",
      "y_train shape (60000,)\n",
      "x_test shape (10000, 28, 28)\n",
      "y_test shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('Initial example dimensions')\n",
    "print(\"x_train shape\", x_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"x_test shape\", x_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "In practice, one should normalize images for convenience in computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "new_y_train = y_train.reshape(60000)\n",
    "new_y_test = y_test.reshape(10000)\n",
    "normalized_x_train = x_train / 255\n",
    "normalized_x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dimensions\n",
      "flat_X_train shape: (60000, 784)\n",
      "flat_X_test shape: (10000, 784)\n",
      "new_Y_train shape: (60000,)\n",
      "new_Y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"New dimensions\")\n",
    "print(\"flat_X_train shape:\", normalized_x_train.shape)\n",
    "print(\"flat_X_test shape:\", normalized_x_test.shape)\n",
    "print(\"new_Y_train shape:\", new_y_train.shape)\n",
    "print(\"new_Y_test shape:\", new_y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification using Linear Regression\n",
    "In this model, I classify whether an image is a  number or not. Here, I am detecting images whether it is a number 1 or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = normalized_x_train\n",
    "testing = normalized_x_test\n",
    "labels = np.where(y_train==0,1,-1)\n",
    "testlabel = np.where(y_test==0,1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 785       \n",
      "=================================================================\n",
      "Total params: 785\n",
      "Trainable params: 785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/10000 [==============================] - 0s 25us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10705387117266656, 0.8924999833106995]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.optimizers import SGD\n",
    "import numpy.random as npr\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(784,), activation='linear'))\n",
    "model.summary()\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=.01)\n",
    "\n",
    "model.compile(optimizer=opt, loss='mse', metrics=['accuracy'])\n",
    "history = model.fit(training, labels, epochs=70, batch_size=2200,verbose=0)\n",
    "w = model.layers[0].get_weights()\n",
    "loss_values = history.history['loss']\n",
    "epochs = range(1, len(loss_values)+1)\n",
    "\n",
    "model.evaluate(x=testing,y=testlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows the loss function graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss_values, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the results using a general Linear Regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression(fit_intercept=True).fit(training, labels)\n",
    "y_pred = reg.predict(testing)\n",
    "\n",
    "# acc_LR = r2_score(testing,y_pred)\n",
    "acc_LR = accuracy_score(testlabel,y_pred.round(), normalize=True)\n",
    "print('Accuracy:',acc_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general Linear Regression method already finds the best possible linear seperable line with coefficient vector $\\hat{\\textbf{w}}$ by using the following equation. \n",
    "\n",
    "$$ \\hat{\\textbf{w}} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}$$\n",
    "\n",
    "where $\\textbf{X}$ represents the training data and $\\textbf{y}$ as the target label vector. In a way, this equation finds the global minumum of the loss function.\n",
    "The NN method has to optmize its weights, or the slope and y-intercept value of the line in order to generate the best seperable line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification using Support Vector Machine (SVM)\n",
    "To make this, we use a Ridge Regression, or an l2-regularizer. We also use a hinge loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = normalized_x_train\n",
    "labels = np.where(y_train==0,1,-1)\n",
    "testlabel = np.where(y_test==0,1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_linsvm = Sequential()\n",
    "model_linsvm.add(Dense(1, input_shape=(784,), activation='softsign',kernel_initializer=RandomUniform(minval=-.2, maxval=.2),bias_initializer=RandomUniform(minval=-.1, maxval=.1),kernel_regularizer = keras.regularizers.l2(l=.001)))\n",
    "# model_linsvm.add(Dense(1,kernel_regularizer = keras.regularizers.l2(l=0.5/4), activation='softsign'))\n",
    "\n",
    "learning_rate = 0.05\n",
    "batch_size = 3000\n",
    "epochs = 100\n",
    "model_linsvm.compile(optimizer=keras.optimizers.SGD(lr=learning_rate,momentum=.01), loss='hinge', metrics=['accuracy'])\n",
    "history = model_linsvm.fit(training, labels, epochs=epochs, batch_size=batch_size,verbose=1,shuffle=False)\n",
    "model.evaluate(x=testing,y=testlabel)\n",
    "w = model.layers[0].get_weights()\n",
    "loss_values = history.history['loss']\n",
    "valloss_values = history.history['val_loss']\n",
    "epochs = range(1, len(loss_values)+1)\n",
    "\n",
    "model_linsvm.evaluate(x=x_test, y=testlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss_values, label='Training Loss')\n",
    "plt.plot(epochs, valloss_values, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this results using the regular SVM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = svm.SVC(kernel='poly',verbose=0)\n",
    "clf.fit(training, labels)\n",
    "y_pred = clf.predict(testing)\n",
    "print('Acc:', accuracy_score(y_pred, testlabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Classification problem using SVM\n",
    "In a multi-class case, the SVM-NN algortihm can generate a nonlinear separable curve. We can extend our current understaning in the perceptron model by adding addional layers. The first layer are input nodes. We will then introduce nonlinearity by applying a Ridge Regession, also called l2 regularizer, and a Rectified Linear Unit (ReLU) activation function. Each node will then be passed through a Linear-SVM algorithm. We classify examples by selecting the highest values in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-class classification requires addional steps. A process called *One-Hot encoding* is used to label multi-class datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(50, input_shape=(784,), activation='relu',kernel_initializer=RandomUniform(minval=-.2, maxval=.2),bias_initializer=RandomUniform(minval=-.1, maxval=.1),kernel_regularizer = keras.regularizers.l2(l=0.125/4))])\n",
    "model.add(Dense(10, input_shape=(50,), activation='softmax'))\n",
    "learning_rate = 0.07\n",
    "epochs = 100\n",
    "opt = SGD(lr=learning_rate)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "# batch_size = np.int32(5/(learning_rate**2))\n",
    "batch_size = 500\n",
    "print(batch_size)\n",
    "history = model.fit(training, labels, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "model.evaluate(x=testing,y=testlabel)\n",
    "w = model.layers[0].get_weights()\n",
    "loss_values = history.history['loss']\n",
    "valloss_values = history.history['val_loss']\n",
    "epochs = range(1, len(loss_values)+1)\n",
    "\n",
    "model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss_values, label='Training Loss')\n",
    "plt.plot(epochs, valloss_values, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below uses the SVM algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.9771\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = svm.SVC(kernel='poly',verbose=0)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print('Acc:', accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general SVM algorithm classifies examples very well. However, this method is computationally heavy for large datasets. Using a Neural Network that imitates SVM can reduce the taxing computations, while using the technique of it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
