{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network models using the CIFAR-10 dataset\n",
    "The Canadian Institute for Advanced Research (CIFAR-10) dataset is a collection of colored images. There are ten different classes: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. No data augmentations are used to focues on the basic NN algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Activation\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "import numpy.random as npr\n",
    "from keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.utils import to_categorical\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the dimensions of the training and testing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial example dimensions\n",
      "X_train shape (50000, 32, 32, 3)\n",
      "Y_train shape (50000, 1)\n",
      "X_test shape (10000, 32, 32, 3)\n",
      "Y_test shape (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Initial example dimensions')\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"Y_train shape\", Y_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"Y_test shape\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Image nomalization and vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train = X_train / 255\n",
    "new_X_test = X_test / 255\n",
    "\n",
    "new_X_train = new_X_train.reshape(50000,32*32*3)\n",
    "new_X_test = new_X_test.reshape(10000,32*32*3)\n",
    "new_Y_train = Y_train.reshape(50000)\n",
    "new_Y_test = Y_test.reshape(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dimensions\n",
      "flat_X_train shape: (50000, 3072)\n",
      "flat_X_test shape: (10000, 3072)\n",
      "new_Y_train shape: (50000,)\n",
      "new_Y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"New dimensions\")\n",
    "print(\"flat_X_train shape:\", new_X_train.shape)\n",
    "print(\"flat_X_test shape:\", new_X_test.shape)\n",
    "print(\"new_Y_train shape:\", new_Y_train.shape)\n",
    "print(\"new_Y_test shape:\", new_Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification using Linear Regression\n",
    "In this model, I classify whether an image is a  cat or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = new_X_test\n",
    "training = new_X_train\n",
    "labels = np.where(Y_train==3,1,-1)\n",
    "testlabel = np.where(Y_test==3,1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.SGD(lr=.001)\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(3072,), activation='linear',kernel_initializer=RandomUniform(minval=-.2, maxval=.2, seed=2),bias_initializer=RandomUniform(minval=-.1, maxval=.1)))\n",
    "model.compile(optimizer=opt, loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(training, labels, epochs=100, batch_size=64,verbose=0)\n",
    "model.evaluate(x=testing,y=testlabel)\n",
    "w = model.layers[0].get_weights()\n",
    "loss_values = history.history['loss']\n",
    "valloss_values = history.history['val_loss']\n",
    "epochs = range(1, len(loss_values)+1)\n",
    "\n",
    "model.evaluate(x=testing,y=testlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss_values, label='Training Loss')\n",
    "plt.plot(epochs, valloss_values, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the results using a general Linear Regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.8429\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "reg = LinearRegression(fit_intercept=True).fit(training, labels)\n",
    "y_pred = reg.predict(testing)\n",
    "\n",
    "# acc_LR = r2_score(testing,y_pred)\n",
    "acc_LR = accuracy_score(testlabel,y_pred.round(), normalize=True)\n",
    "print('Acc:',acc_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification using Support Vector Machine (SVM)\n",
    "To make this, we use a Ridge Regression, or an l2-regularizer. We also use a hinge loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = new_X_test\n",
    "training = new_X_train\n",
    "labels = np.where(Y_train==3,1,-1)\n",
    "testlabel = np.where(Y_test==3,1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 2s 33us/step - loss: 6.7328 - accuracy: 0.0984\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 1s 17us/step - loss: 5.8753 - accuracy: 0.4161\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 1s 16us/step - loss: 5.0458 - accuracy: 0.8912\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 1s 16us/step - loss: 4.8227 - accuracy: 0.8980\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 1s 16us/step - loss: 4.6260 - accuracy: 0.8990\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 1s 16us/step - loss: 4.4423 - accuracy: 0.8994\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 1s 16us/step - loss: 4.2685 - accuracy: 0.8997\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 1s 17us/step - loss: 4.1030 - accuracy: 0.8998\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 1s 16us/step - loss: 3.9451 - accuracy: 0.8998\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 1s 17us/step - loss: 3.7941 - accuracy: 0.8999\n",
      "10000/10000 [==============================] - 0s 48us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.719522741699219, 0.8999000191688538]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_linsvm = Sequential()\n",
    "opt_linsvm = keras.optimizers.SGD(lr=.0005)\n",
    "model_linsvm.add(Dense(1, input_shape=(3072,), activation='softsign',kernel_initializer=RandomUniform(minval=-.2, maxval=.2, seed=2),bias_initializer=RandomUniform(minval=-.1, maxval=.1),kernel_regularizer=keras.regularizers.l2(.5/4)))\n",
    "model_linsvm.compile(optimizer=opt_linsvm, loss='hinge', metrics=['accuracy'])\n",
    "model_linsvm.fit(training, labels, epochs=10, batch_size=300)\n",
    "\n",
    "model_linsvm.evaluate(x=testing,y=testlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this results using the regular SVM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = svm.SVC(kernel='poly',verbose=0)\n",
    "clf.fit(training, labels)\n",
    "y_pred = clf.predict(testing)\n",
    "print('Acc:', accuracy_score(y_pred, testlabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can modify this approach by adding another layer that introduces linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm = Sequential()\n",
    "model_svm.add(Dense(50, input_shape=(3072,), activation='relu'))\n",
    "model_svm.add(Dense(1, activation='linear',kernel_regularizer=keras.regularizers.l2(.5)))\n",
    "\n",
    "learning_rate = 0.04\n",
    "batch_size = np.int32(20/(learning_rate**2))\n",
    "print('Batch size:',batch_size)\n",
    "\n",
    "model_svm.compile(optimizer=keras.optimizers.SGD(lr=learning_rate), loss='hinge', metrics=['accuracy'])\n",
    "model_svm.fit(training, labels, epochs=50, batch_size=batch_size,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Classification problem using SVM\n",
    "In a multi-class case, the SVM-NN algortihm can generate a nonlinear separable curve. We can extend our current understaning in the perceptron model by adding addional layers. The first layer are input nodes. We will then introduce nonlinearity by applying a Ridge Regession, also called l2 regularizer, and a Rectified Linear Unit (ReLU) activation function. Each node will then be passed through a Linear-SVM algorithm. We classify examples by selecting the highest values in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "new_X_train = rgb2gray(X_train) \n",
    "new_X_test = rgb2gray(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train = new_X_train.reshape(50000,32*32)\n",
    "new_X_test = new_X_test.reshape(10000,32*32)\n",
    "new_Y_train = Y_train.reshape(50000)\n",
    "new_Y_test = Y_test.reshape(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-class classification requires addional steps. A process called *One-Hot encoding* is used to label multi-class datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_Y_train = to_categorical(Y_train,10)\n",
    "encoded_Y_test = to_categorical(Y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "opt = keras.optimizers.SGD(lr=.1,momentum=0.01,nesterov=True,decay = .001/ epochs)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(15, input_shape=(1024,), activation='relu',kernel_initializer=RandomUniform(minval=-.2, maxval=.2, seed=2),bias_initializer=RandomUniform(minval=-.1, maxval=.1)))\n",
    "model.add(Dense(10,input_shape=(50,),kernel_regularizer = keras.regularizers.l2(l=.125/8), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "training = new_X_train\n",
    "testing = new_X_test\n",
    "labels = encoded_Y_train\n",
    "testlabel = encoded_Y_test\n",
    "\n",
    "history = model.fit(training, labels, epochs=100, batch_size=100)\n",
    "model.evaluate(x=testing,y=testlabel)\n",
    "w = model.layers[0].get_weights()\n",
    "loss_values = history.history['loss']\n",
    "valloss_values = history.history['val_loss']\n",
    "epochs = range(1, len(loss_values)+1)\n",
    "\n",
    "model.evaluate(x=testing,y=testlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss_values, label='Training Loss')\n",
    "plt.plot(epochs, valloss_values, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below uses the SVM algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = svm.SVC(kernel='poly',verbose=0)\n",
    "clf.fit(training, labels)\n",
    "y_pred = clf.predict(testing)\n",
    "print('Acc:', accuracy_score(y_pred, testlabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This is computationally heavy, so I will not run this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general SVM algorithm classifies examples very well. However, this method is computationally heavy for large datasets. Using a Neural Network that imitates SVM can reduce the taxing computations, while using the technique of it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
